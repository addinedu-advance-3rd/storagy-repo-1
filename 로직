🎯 1. YOLO를 활용한 탐지
**홈(공구 자리)**는 바운딩 박스(BBox)로 탐지
**공구(드라이버, 스패너, 망치)**는 세그멘테이션(Segmentation)으로 탐지
🎯 2. "공구가 홈 안에 있는지" 판단하는 방식
🔹 공구의 위치와 홈의 위치를 비교하여 **"홈 안에 공구가 있다면 반납된 상태, 없다면 가져간 상태"**로 판단 가능
🔹 두 가지 방법이 있음:

✅ 방법 1: Intersection over Union (IOU) 활용 (바운딩 박스 중심)
📌 개념

공구의 세그멘테이션 영역과 홈의 바운딩 박스가 얼마나 겹치는지(IOU)를 계산
IOU 값이 일정 기준(예: 0.5 이상)이면 "홈 안에 있다"로 판단
IOU 값이 낮으면 "공구가 사라졌다"로 판단
📌 예제 로직

홈의 바운딩 박스 좌표 (home_bbox) 가져오기
공구의 세그멘테이션 결과에서 바운딩 박스 (tool_bbox) 가져오기
IOU 계산
IOU = (겹친 영역) / (전체 영역)
기준값(예: 0.5) 이상이면 홈 안에 있는 것으로 판단
✅ 방법 2: 픽셀 기반 분석 (세그멘테이션 중심)
📌 개념

세그멘테이션 결과에서 공구의 픽셀들이 홈의 바운딩 박스 내부에 얼마나 포함되는지 계산
공구 픽셀의 80% 이상이 홈 내부에 있으면 "홈 안에 있다"로 판단
공구 픽셀이 거의 없으면 "공구가 사라졌다"로 판단
📌 예제 로직

홈의 바운딩 박스 좌표(home_bbox) 가져오기
공구의 세그멘테이션 마스크(tool_mask) 가져오기
공구 마스크 픽셀 중 홈 바운딩 박스 내부 픽셀의 비율 계산
기준값(예: 80%) 이상이면 홈 안에 있는 것으로 판단
🔹 구현 가능성
🛠️ 현실적으로 적용 가능!

YOLO의 바운딩 박스 + 세그멘테이션 출력 결과를 활용하면, 홈과 공구의 관계를 실시간으로 분석 가능
IOU 기반 방식은 계산이 단순하고 빠름
픽셀 기반 방식은 공구가 겹치는 경우에도 더 정밀한 판단 가능
🔹 최종 결론
✔ "홈 바운딩 박스 vs. 공구 세그멘테이션" 비교하여 가져감/반납 상태 확인 가능
✔ IOU(Intersection Over Union) 또는 픽셀 기반 분석을 활용하면 정확하게 구현 가능
✔ 현재 기술(YOLO + 공간적 관계 분석)로 충분히 구현 가능 🎯🚀

💡 결론: "공구가 홈 안에 있는지 여부를 판단하는 것"은 실제 구현 가능한 로직이며, 정확도를 높일 수 있는 방법도 존재함! 🔥


✅ 2️⃣ Preprocessing (전처리)
🔹 추천 설정:
✅ Auto-Orient (ON) → 가로/세로 방향이 맞지 않는 이미지 자동 보정
✅ Resize (640x640) → YOLOv8 기본 입력 크기 (Stretch 대신 Resize with Padding 추천)

✔ Stretch to 640x640 (X) → YOLO는 비율을 유지하는 것이 중요함 → Resize with Padding이 더 적절
✔ Resize with Padding을 선택하면, 비율을 유지한 채 640x640으로 맞춤

✅ 3️⃣ Augmentation (데이터 증강)
💡 데이터 207장이므로, 약간의 증강을 적용하는 것이 좋습니다.

🔹 추천 Augmentation 설정:
✅ Flip (좌우 반전) → 공구 방향이 바뀌어도 감지되도록 학습
✅ Brightness & Contrast (밝기/대비 조정 ±15%) → 조명이 달라져도 인식 가능하게
✅ Rotation (최대 ±10도) → 살짝 기울어진 공구도 학습
✅ Blur (약간의 블러 추가) → 모션 블러에도 강한 모델 만들기
